/*
 * Optimize the neon implementation which is generated by GCC12
 * or Clang by the commit: https://github.com/openzfs/zfs/pull/14234

 * Due to the compiler limitation, the GCC version before 12 will
 * produce not optimized code for Aarch64, so use the asm function
 * to accelerate the performance here.
 * Copyright (C) 2022 Kevin Zhao.
 *
 * Authors:
 *	Kevin Zhao <kevin.zhao@linaro.org>
 * Co-authored-by:
 * Richard Yao <richard.yao@alumni.stonybrook.edu>
 *
 * This software is available to you under a choice of one of two
 * licenses.  You may choose to be licensed under the terms of the GNU
 * General Public License (GPL) Version 2, available from the file
 * COPYING in the main directory of this source tree, or the
 * OpenIB.org BSD license below:
 *
 *     Redistribution and use in source and binary forms, with or
 *     without modification, are permitted provided that the following
 *     conditions are met:
 *
 *	- Redistributions of source code must retain the above
 *	  copyright notice, this list of conditions and the following
 *	  disclaimer.
 *
 *	- Redistributions in binary form must reproduce the above
 *	  copyright notice, this list of conditions and the following
 *	  disclaimer in the documentation and/or other materials
 *	  provided with the distribution.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 * SOFTWARE.
 */

	.section .text
	.arch armv8-a
	.globl	fletcher_4_aarch64_neon_native
	.type	fletcher_4_aarch64_neon_native, @function
fletcher_4_aarch64_neon_native:
        .cfi_startproc
        ldp     q4, q0, [x0]
        add     x8, x1, x2
        ldp     q5, q1, [x0, #32]
        ldp     q6, q2, [x0, #64]
        ldp     q7, q3, [x0, #96]
.LBB0_1:
        ldr     q16, [x1], #16
        cmp     x1, x8
        uaddw2  v0.2d, v0.2d, v16.4s
        uaddw   v4.2d, v4.2d, v16.2s
        add     v1.2d, v0.2d, v1.2d
        add     v5.2d, v4.2d, v5.2d
        add     v2.2d, v1.2d, v2.2d
        add     v6.2d, v5.2d, v6.2d
        add     v3.2d, v2.2d, v3.2d
        add     v7.2d, v6.2d, v7.2d
        b.lo    .LBB0_1
        stp     q4, q0, [x0]
        stp     q5, q1, [x0, #32]
        stp     q6, q2, [x0, #64]
        stp     q7, q3, [x0, #96]
        ret
.Lfunc_end1:
	.size	fletcher_4_aarch64_neon_native, .Lfunc_end1-fletcher_4_aarch64_neon_native
	.cfi_endproc

	.text
	.globl	fletcher_4_aarch64_neon_byteswap
	.type	fletcher_4_aarch64_neon_byteswap,@function
fletcher_4_aarch64_neon_byteswap:       // @fletcher_4_aarch64_neon_byteswap
        .cfi_startproc
        ldp     q4, q0, [x0]
        add     x8, x1, x2
        ldp     q5, q1, [x0, #32]
        ldp     q6, q2, [x0, #64]
        ldp     q7, q3, [x0, #96]
.LBB1_1:                                // =>This Inner Loop Header: Depth=1
        ldr     q16, [x1], #16
        cmp     x1, x8
        rev32   v16.16b, v16.16b
        uaddw2  v0.2d, v0.2d, v16.4s
        uaddw   v4.2d, v4.2d, v16.2s
        add     v1.2d, v0.2d, v1.2d
        add     v5.2d, v4.2d, v5.2d
        add     v2.2d, v1.2d, v2.2d
        add     v6.2d, v5.2d, v6.2d
        add     v3.2d, v2.2d, v3.2d
        add     v7.2d, v6.2d, v7.2d
        b.lo    .LBB1_1
        stp     q4, q0, [x0]
        stp     q5, q1, [x0, #32]
        stp     q6, q2, [x0, #64]
        stp     q7, q3, [x0, #96]
        ret
.Lfunc_end2:
	.size	fletcher_4_aarch64_neon_byteswap, .Lfunc_end1-fletcher_4_aarch64_neon_byteswap
	.cfi_endproc
